[
["index.html", "Inference in Six Lessons Chapter 1 Inference", " Inference in Six Lessons Carlisle Rainey 2020-12-01 Chapter 1 Inference King, Keohane, and Verba (1996, p. 46) define inference as “the process of using the facts we know to learn about facts we do not know.” Consider the following three targets of inference: The Average Treatment Effect. Suppose you conduct an experiment in which you assign \\(N\\) subjects to either treatment or control. For each subject \\(n\\), you observe either the outcome under the treatment condition \\(Y^{T}_n\\) or the outcome under control \\(Y^{C}_n\\). We define the average treatment effect or ATE as \\(\\displaystyle \\frac{1}{N}\\sum_{n = 1}^{N}\\left( Y^{T}_n - Y^{C}_n \\right)\\). However, because we cannot place subject \\(n\\) in both treatment and control, we cannot observe the ATE; we can only estimate it. Features of a Population, Using a Sample. Suppose we have a random sample of \\(N\\) observations from a much larger population. Then we can use the sample to estimate features of the population, such as the average of a variable or correlation between two variables. However, because we cannot (or perhaps choose to not) observe the each member of population, we cannot observe the features of the population directly. Parameters of a Stochastic Model. Suppose the outcome variable \\(y\\) is a collection of samples from a distribution \\(f(\\theta)\\). Then we cannot observe \\(\\theta\\) directly, but we can use the observed samples \\(y\\) to learn estimate \\(\\theta\\). For example, suppose you have a binary outcome variable \\(y\\) that you model as draws from a Bernoulli distribution. Then \\(y_i \\sim \\text{Bernoulli}(\\pi)\\). Your inferential target would not be the proportion of ones in the sample, but the value of \\(\\pi\\). Modeling these three targets of inference identical in some situations and very similar in many. For simplicity, I focus on estimating the parameters of a stochastic model. In this situation, we observe a sample \\(y\\) from a particular distribution and use the sample to estimate the parameters of the distribution.S We consider two types of estimates: Point Estimates: using the observed data to calculate a single value or best-guess for the unobservable quantity of interest. Interval Estimates: using the observed data to calculate a range of values for the unobservable quantity of interest. For each type of estimate, we consider: How to develop an estimator. How to evaluate an estimator. "],
["deriving-point-estimates.html", "Chapter 2 Deriving Point Estimates", " Chapter 2 Deriving Point Estimates In this chapter, I discuss three methods to derive point estimators of quantities of interest: , Bayes’ inference, method of moments (briefly), and maximum likelihood. "],
["bayesian-inference.html", "2.1 Bayesian Inference", " 2.1 Bayesian Inference Bayesian inference follows a simple recipe: Choose a distribution for the data. Choose a distribution to describe your prior beliefs. Update the prior distribution upon observing the data by computing the posterior distribution. 2.1.1 Mechanics Suppose a random sample from a distribution \\(f(x; \\theta)\\) that depends on the unknown parameter \\(\\theta\\). Bayesian inference models our beliefs about the unknown parameter \\(\\theta\\) as a distribution and answers the question: what should we believe about \\(\\theta\\), given the observed samples \\(x = \\{x_1, x_2, ..., x_n\\}\\) from \\(f(x; \\theta)\\). These beliefs are simply the conditional distribution \\(f(\\theta \\mid x)\\). By Bayes’ rule, \\(\\displaystyle f(\\theta \\mid x) = \\frac{f(x \\mid \\theta)f(\\theta)}{f(x)} = \\frac{f(x \\mid \\theta)f(\\theta)}{\\displaystyle \\int_{-\\infty}^\\infty f(x \\mid \\theta)f(\\theta) d\\theta}\\). \\[ \\displaystyle \\underbrace{f(\\theta \\mid x)}_{\\text{posterior}} = \\frac{\\overbrace{f(x \\mid \\theta)}^{\\text{likelihood}} \\times \\overbrace{f(\\theta)}^{\\text{prior}}}{\\displaystyle \\underbrace{\\int_{-\\infty}^\\infty f(x \\mid \\theta)f(\\theta) d\\theta}_{\\text{normalizing constant}}} \\] There are four parts to a Bayesian analysis. \\(f(\\theta \\mid x)\\). “The posterior;” what we’re trying to find. This distribution models our beliefs about parameter \\(\\theta\\) given the data \\(x\\). \\(f(x \\mid \\theta)\\). “The likelihood.” This distribution model conditional density/probability of the data \\(x\\) given the parameter \\(\\theta\\). We need to invert the conditioning in order to find the posterior. \\(f(\\theta)\\). “The prior;” our beliefs about \\(\\theta\\) prior to observing the sample \\(x\\). \\(f(x) =\\int_{-\\infty}^\\infty f(x \\mid \\theta)f(\\theta) d\\theta\\). A normalizing constant. Recall that the role of the normalizing constant is to force the distribution to integrate or sum to one. Therefore, we can safely ignore this constant until the end, and then find proper normalizing constant. It’s convenient to choose a conjugate prior distribution that, when combined with the likelihood, produces a posterior from the same family. As a running example, we use the toothpaste cap problem: We have a toothpaste cap–one with a wide bottom and a narrow top. We’re going to toss the toothpaste cap. It can either end up lying on its side, its (wide) bottom, or its (narrow) top. We want to estimate the probability of the toothpaste cap landing on its top. We can model each toss as a Bernoulli trial, thinking of each toss as a random variable \\(X\\) where \\(X \\sim \\text{Bernoulli}(\\pi)\\). If the cap lands on its top, we think of the outcome as 1. If not, as 0. Suppose we toss the cap \\(N\\) times and observe \\(k\\) tops. What is the posterior distribution of \\(\\pi\\)? 2.1.1.1 The Likelihood According to the model \\(f(x_i \\mid \\pi) = \\pi^{x_i} (1 - \\pi)^{(1 - x_i)}\\). Because the samples are iid, we can find the joint distribution \\(f(x) = f(x_1) \\times ... \\times f(x_N) = \\prod_{i = 1}^N f(x_i)\\). We’re just multiplying \\(k\\) \\(\\pi\\)s (i.e., each of the \\(k\\) ones has probability \\(\\pi\\)) and \\((N - k)\\) \\((1 - \\pi)\\)s (i.e., each of the \\(N - k\\) zeros has probability \\(1 - \\pi\\)), so that the \\(f(x | \\pi) = \\pi^{k} (1 - \\pi)^{(N - k)}\\). \\[ \\text{the likelihood: } f(x | \\pi) = \\pi^{k} (1 - \\pi)^{(N - k)}, \\text{where } k = \\sum_{n = 1}^N x_n \\\\ \\] 2.1.1.2 The Prior The prior describes your beliefs about \\(\\pi\\) before observing the data. Here are some questions that we might ask ourselves the following questions: What’s the most likely value of \\(\\pi\\)? About 0.15. Are our beliefs best summarizes by a distribution that’s skewed to the left or right? To the right. \\(\\pi\\) is about _____, give or take _____ or so. Perhaps 0.17 and 0.10. There’s a 25% chance that \\(\\pi\\) is less than ____. Perhaps 0.05. There’s a 25% chance that \\(\\pi\\) is greater than ____. Perhaps 0.20. Given these answers, we can sketch the pdf of the prior distribution for \\(\\pi\\). Now we need to find a density function that matches these prior beliefs. For this Bernoulli model, the beta distribution is the conjugate prior. While a conjugate prior is not crucial in general, it makes the math much more tractable. So then what beta distribution captures our prior beliefs? There’s a code snippet here to help you explore different beta distributions. After some exploration, we find that setting the parameters \\(\\alpha\\) and \\(\\beta\\) of the beta distribution to 3 and 15, respectively, captures our prior beliefs about the probability of getting a top. ggplot(data_frame(x = seq(0, 1, length.out = 100))) + stat_function(fun = dbeta, args = list(shape1 = 3, shape2 = 15)) + labs(title = &quot;pdf of the beta(3, 15) distribution&quot;, x = &quot;pi&quot;, y = &quot;prior density&quot;) ## Warning: `data_frame()` is deprecated as of tibble 1.1.0. ## Please use `tibble()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. The pdf of the beta distribution is \\(f(x) = \\frac{1}{B(\\alpha, \\beta)} x^{\\alpha - 1}(1 - x)^{\\beta - 1}\\). Remember that \\(B()\\) is the beta function, so \\(\\frac{1}{B(\\alpha, \\beta)}\\) is a constant. Let’s denote our chosen values of \\(\\alpha = 3\\) and \\(\\beta = 15\\) as \\(\\alpha^*\\) and \\(\\beta^*\\). As we see in a moment, it’s convenient distinguish the parameters in the prior distribution from other parameters. \\[ \\text{the prior: } f(\\pi) = \\frac{1}{B(\\alpha^*, \\beta^*)} \\pi^{\\alpha^* - 1}(1 - \\pi)^{\\beta^* - 1} \\] 2.1.1.3 The Posterior Now we need to compute the posterior by multiplying the likelihood times the prior and then finding the normalizing constant. \\[ \\text{the posterior: } \\displaystyle \\underbrace{f(\\pi \\mid x)}_{\\text{posterior}} = \\frac{\\overbrace{f(x \\mid \\pi)}^{\\text{likelihood}} \\times \\overbrace{f(\\pi)}^{\\text{prior}}}{\\displaystyle \\underbrace{\\int_{-\\infty}^\\infty f(x \\mid \\pi)f(\\pi) d\\pi}_{\\text{normalizing constant}}} \\\\ \\] Now we plug in the likelihood, plug in the prior, and denote the normalizing constant as \\(C_1\\) to remind ourselves that it’s just a constant. \\[ \\text{the posterior: } \\displaystyle \\underbrace{f(\\pi \\mid x)}_{\\text{posterior}} = \\frac{\\overbrace{\\left[ \\pi^{k} (1 - \\pi)^{(N - k) }\\right] }^{\\text{likelihood}} \\times \\overbrace{ \\left[ \\frac{1}{B(\\alpha^*, \\beta^*)} \\pi^{\\alpha^* - 1}(1 - \\pi)^{\\beta^* - 1} \\right] }^{\\text{prior}}}{\\displaystyle \\underbrace{C_1}_{\\text{normalizing constant}}} \\\\ \\] Now we need to simplify the right-hand side. First, notice that the term \\(\\frac{1}{B(\\alpha^*, \\beta^*)}\\) in the numerator is just a constant. We can incorporate that constant term with \\(C_1\\) by multiplying top and bottom by \\(B(\\alpha^*, \\beta^*)\\) and letting \\(C_2 = C_1 \\times B(\\alpha^*, \\beta^*)\\). \\[ \\text{the posterior: } \\displaystyle \\underbrace{f(\\pi \\mid x)}_{\\text{posterior}} = \\frac{\\overbrace{\\left[ \\pi^{k} (1 - \\pi)^{(N - k) }\\right] }^{\\text{likelihood}} \\times \\left[ \\pi^{\\alpha^* - 1}(1 - \\pi)^{\\beta^* - 1} \\right] }{\\displaystyle \\underbrace{C_2}_{\\text{new normalizing constant}}} \\\\ \\] Now we can collect the exponents with base \\(\\pi\\) and the exponents with base \\((1 - \\pi)\\). \\[ \\text{the posterior: } \\displaystyle \\underbrace{f(\\pi \\mid x)}_{\\text{posterior}} = \\frac{\\left[ \\pi^{k} \\times \\pi^{\\alpha^* - 1} \\right] \\times \\left[ (1 - \\pi)^{(N - k) } \\times (1 - \\pi)^{\\beta^* - 1} \\right] }{ C_2} \\\\ \\] Recalling that \\(x^a \\times x^b = x^{a + b}\\), we combine the powers. \\[ \\text{the posterior: } \\displaystyle \\underbrace{f(\\pi \\mid x)}_{\\text{posterior}} = \\frac{\\left[ \\pi^{(\\alpha^* + k) - 1} \\right] \\times \\left[ (1 - \\pi)^{[\\beta^* + (N - k)] - 1} \\right] }{ C_2} \\\\ \\] Because we’re clever, we notice that this is almost a beta distribution with \\(\\alpha = (\\alpha^* + k)\\) and \\(\\beta = [\\beta^* + (N - k)]\\). If \\(C_2 = B(\\alpha^* + k, \\beta^* + (N - k))\\), then the posterior is exactly a \\(\\text{beta}(\\alpha^* + k, \\beta^* + [N - k]))\\) distribution. This is completely expected. We chose a beta distribution for the prior because it would give us a beta posterior distribution. For simplicity, we can denote the parameter for the beta posterior as \\(\\alpha^\\prime\\) and \\(\\beta^\\prime\\), so that \\(\\alpha^\\prime = \\alpha^* + k\\) and \\(\\beta^\\prime = \\beta^* + [N - k]\\) \\[ \\begin{aligned} \\text{the posterior: } \\displaystyle \\underbrace{f(\\pi \\mid x)}_{\\text{posterior}} &amp;= \\frac{ \\pi^{\\overbrace{(\\alpha^* + k)}^{\\alpha^\\prime} - 1} \\times (1 - \\pi)^{\\overbrace{[\\beta^* + (N - k)]}^{\\beta^\\prime} - 1} }{ B(\\alpha^* + k, \\beta^* + [N - k])} \\\\ &amp;= \\frac{ \\pi^{\\alpha^\\prime - 1} \\times (1 - \\pi)^{\\beta^\\prime - 1} }{ B(\\alpha^\\prime, \\beta^\\prime)}, \\text{where } \\alpha^\\prime = \\alpha^* + k \\text{ and } \\beta^\\prime = \\beta^* + [N - k] \\end{aligned} \\] This is an elegant, simple solution. To obtain the parameters for the beta posterior distribution, we just add the number of tops (Bernoulli successes) to the prior value for \\(\\alpha\\) and the number of not-tops (sides and bottoms; Bernoulli failures) to the prior value for \\(\\beta\\). Suppose that I tossed the toothpaste cap 150 times and got 8 tops. # prior parameters alpha_prior &lt;- 3 beta_prior &lt;- 15 # data k &lt;- 8 N &lt;- 150 # posterior parameters alpha_posterior &lt;- alpha_prior + k beta_posterior &lt;- beta_prior + N - k # plot prior and posterior gg_prior &lt;- ggplot() + stat_function(fun = dbeta, args = list(shape1 = alpha_prior, shape2 = beta_prior)) + labs(title = &quot;prior distribution&quot;, x = &quot;pi&quot;, y = &quot;prior density&quot;) gg_posterior &lt;- ggplot() + stat_function(fun = dbeta, args = list(shape1 = alpha_posterior, shape2 = beta_posterior)) + labs(title = &quot;posterior distribution&quot;, x = &quot;pi&quot;, y = &quot;posterior density&quot;) library(patchwork) gg_prior + gg_posterior 2.1.1.4 Bayesian Point Estimates In this section, we want point estimates—a best guess at the parameter—not a full posterior distribution. We have three options: The posterior mean. The posterior mean minimizes a squared-error loss function. That is, the cost of guessing \\(a\\) when the truth is \\(\\alpha\\) is \\((a - \\alpha)^2\\). In the case of the beta posterior, it’s just \\(\\dfrac{\\alpha^\\prime}{\\alpha^\\prime + \\beta^\\prime}\\). For our prior and data, we have \\(\\dfrac{3 + 8}{(3 + k) + (15 + 150 - 8)} \\approx 0.065\\). The posterior median: The posterior median minimizes an absolute loss function where the cost of guessing \\(a\\) when the truth is \\(\\alpha\\) is \\(|a - \\alpha|\\). Intuitively, there’s a 50% chance that \\(\\pi\\) falls above and below the posterior median. In the case of the beta posterior, it’s just \\(\\dfrac{\\alpha^\\prime - \\frac{1}{3}}{\\alpha^\\prime + \\beta^\\prime - \\frac{2}{3}}\\) (for \\(\\alpha^\\prime, \\beta^\\prime &gt; 1\\)). For our prior and data, we have \\(\\dfrac{3 + 8 -\\frac{1}{3}}{(3 + k) + (15 + 150 - 8) - \\frac{2}{3}} \\approx 0.064\\). The posterior mode: The posterior mode is the most likely value of \\(\\pi\\), so it minimizes a loss function that penalizes all misses equally. In the case of the beta posterior, it’s just \\(\\dfrac{\\alpha^\\prime - 1}{\\alpha^\\prime + \\beta^\\prime - 2}\\) (for \\(\\alpha^\\prime, \\beta^\\prime &gt; 1\\)). For our prior and data, we have \\(\\dfrac{3 + 8 - 1}{(3 + k) + (15 + 150 - 8) - 2} \\approx 0.060\\). 2.1.2 Example: Poisson Distribution Suppose we collect \\(N\\) random samples \\(x = \\{x_1, x_2, ..., x_N\\}\\) and model each draw as a random variable \\(X \\sim \\text{Poisson}(\\lambda)\\). Find the posterior distribution of \\(\\lambda\\) for the gamma prior distribution. Hint: the gamma distribution is the conjugate prior for the Poisson likelihood. \\[ \\begin{aligned} \\text{Poisson likelihood: } f(x \\mid \\lambda) &amp;= \\prod_{n = 1}^N \\frac{\\lambda^{x_n} e^{-\\lambda}}{x_n!} \\\\ &amp;= \\displaystyle \\left[ \\frac{1}{\\prod_{n = 1}^N x_n !} \\right]e^{-N\\lambda}\\lambda^{\\sum_{n = 1}^N x_n} \\end{aligned} \\] \\[ \\text{Gamma prior: } f( \\lambda; \\alpha^*, \\beta^*) = \\frac{{\\beta^*}^{\\alpha^*}}{\\Gamma(\\alpha^*)} \\lambda^{\\alpha^* - 1} e^{-\\beta^*\\lambda} \\] To find the posterior, we multiply the likelihood times the prior and normalize. Because the gamma prior distribution is the conjugate prior for the Poisson likelihood, we know that the posterior will be a gamma distribution. \\[ \\begin{aligned} \\text{Gamma posterior: } f( \\lambda \\mid x) &amp;= \\frac{\\left( \\displaystyle \\left[ \\frac{1}{\\prod_{n = 1}^N x_n !} \\right]e^{-N\\lambda}\\lambda^{\\sum_{n = 1}^N x_n}\\right) \\times \\left( \\left[ \\frac{{\\beta^*}^{\\alpha^*}}{\\Gamma(\\alpha^*)} \\right] \\lambda^{\\alpha^* - 1} e^{-\\beta^*\\lambda}\\right)}{C_1} \\\\ \\end{aligned} \\] Because \\(x\\), \\(\\alpha_*\\), and \\(\\beta\\) are fixed, the terms in square brackets are constant, so we can safely consider those part of the normalizing constant. \\[ \\begin{aligned} &amp;= \\frac{\\left( \\displaystyle e^{-N\\lambda}\\lambda^{\\sum_{n = 1}^N x_n}\\right) \\times \\left( \\lambda^{\\alpha^* - 1} e^{-\\beta^*\\lambda}\\right)}{C_2} \\\\ \\end{aligned} \\] Now we can collect the exponents with the same base. \\[ \\begin{aligned} &amp;= \\frac{\\left( \\lambda^{\\alpha^* - 1} \\times \\lambda^{\\sum_{n = 1}^N x_n}\\right) \\times \\left( \\displaystyle e^{-N\\lambda} \\times e^{-\\beta^*\\lambda} \\right)}{C_2} \\\\ &amp;= \\frac{\\lambda^{ \\overbrace{\\left[ \\alpha^* + \\sum_{n = 1}^N x_n \\right]}^{\\alpha^\\prime} - 1} e^{-\\overbrace{[\\beta^* + N]}^{\\beta^\\prime}\\lambda} }{C_2} \\\\ \\end{aligned} \\] We recognize this as almost a Gamma distribution with parameters \\(\\alpha^\\prime = \\alpha^* + \\sum_{n = 1}^N x_n\\) and \\(\\beta^\\prime = \\beta^* + N\\). Indeed, if \\(\\frac{1}{C_2} = \\frac{{\\beta^\\prime}^{\\alpha^\\prime}}{\\Gamma(\\alpha^{\\prime})}\\), then we have exactly a gamma distribution. \\[ \\begin{aligned} &amp;= \\frac{{\\beta^\\prime}^{\\alpha^\\prime}}{\\Gamma(\\alpha^{\\prime})} \\lambda^{ \\alpha^\\prime - 1} e^{-\\beta^\\prime\\lambda}, \\text{where } \\alpha^\\prime = \\alpha^* + \\sum_{n = 1}^N x_n \\text{ and } \\beta^\\prime = \\beta^* + N \\end{aligned} \\] Like the Bernoulli likelihood with the beta prior, the Poisson likelihood withe the gamma prior gives a nice result. We start with values parameters of the gamma distribution \\(\\alpha = \\alpha^*\\) and \\(\\beta + \\beta^*\\) so that the gamma prior distribution describes our prior beliefs about the parameters \\(\\lambda\\) of the Poisson distribution. Then we add the sum of the data \\(x\\) to \\(\\alpha^*\\) and the number of samples \\(N\\) to \\(\\beta^*\\) to obtain the parameters of the gamma posterior distribution. The code below shows the posterior distribution # set see to make reproducible set.seed(1234) # prior parameters alpha_prior &lt;- 3 beta_prior &lt;- 3 # create an &quot;unknown&quot; value of lambda to estimate lambda &lt;- 2 # generate a data set N &lt;- 5 # number of samples x &lt;- rpois(N, lambda = lambda) print(x) # print the data set ## [1] 0 2 2 2 4 # posterior parameters alpha_posterior &lt;- alpha_prior + sum(x) beta_posterior &lt;- beta_prior + N # plot prior and posterior gg_prior &lt;- ggplot() + xlim(0, 5) + stat_function(fun = dgamma, args = list(shape = alpha_prior, rate = beta_prior)) + labs(title = &quot;prior distribution&quot;, x = &quot;lambda&quot;, y = &quot;prior density&quot;) gg_posterior &lt;- ggplot() + xlim(0, 5) + stat_function(fun = dgamma, args = list(shape = alpha_posterior, rate = beta_posterior)) + labs(title = &quot;posterior distribution&quot;, x = &quot;lambda&quot;, y = &quot;posterior density&quot;) gg_prior + gg_posterior # uses patchwork package # posterior mean: alpha/beta alpha_posterior/beta_posterior ## [1] 1.625 # posterior median: no closed form, so simulate post_sims &lt;- rgamma(10000, alpha_posterior, beta_posterior) median(post_sims) ## [1] 1.586689 # posterior mode: (alpha - 1)/beta for alpha &gt; 1 (alpha_posterior - 1)/beta_posterior ## [1] 1.5 2.1.3 Remarks Bayesian inference presents two difficulties. Choosing a prior. It can be hard to actually construct a prior distribution. It’s challenging when dealing with a single parameter. It becomes much more difficult when dealing with several or many parameters. Priors are subjective, so that one researcher’s prior might not work for another. Computing the posterior. Especially for many-parameter problems and non-conjugate priors, computing the posterior can be nearly intractable. However, there are several practical solutions to these difficulties. Choosing a prior. We can use a “uninformative” or constant prior. Sometimes, we can use an improper prior that doesn’t integrate to one, but places equal prior weight on all values. We can use an extremely diffuse prior. For example, if we wanted to estimate the average height in a population in inches, we might use a normal distribution centered at zero with an SD of 10,000. This prior says: “The average height is about zero, give or take 10,000 inches or so.” We can use an informative prior, but conduct careful robustness checks to assess whether the conclusions depend on the particular prior. We can use a weakly informative prior, that rules places meaningful prior weight on all the plausible values and little prior weight only on the most implausible values. As a guideline, you might create a weakly informative prior by doubling or tripling the SD of the informative prior. Computing the posterior. While analytically deriving the posterior becomes intractable for most applied problems, it’s relatively easy to sample from the posterior distribution for many models. Algorithms like Gibbs samplers, MCMC, and HMC make this sampling procedure straightforward for a given model. Software such as Stan make sampling easy to set up and very fast. Post-processing R packages such as tidybayes make it each to work with the posterior simulations. 2.1.4 Exercises Exercise 2.1 Suppose we collect \\(N\\) random samples \\(x = \\{x_1, x_2, ..., x_N\\}\\) and model each draw as a random variable \\(X \\sim \\text{expontial}(\\lambda)\\) with pdf \\(f(x_n | \\lambda) = \\lambda e^{-\\lambda x_n}\\). Find the posterior distribution of \\(\\lambda\\) for the gamma prior distribution. Hint: the gamma distribution is the conjugate prior for the exponential likelihood. Hint Use the Poisson example from above. Because they share a prior, the math works quite similarly. Solution \\[ \\begin{aligned} \\text{exponential likelihood: } f(x \\mid \\lambda) &amp;= \\prod_{n = 1}^N \\lambda e^{-\\lambda x_n} \\\\ &amp;= \\lambda^N e^{-\\lambda \\sum_{n = 1}^N x_n} \\end{aligned} \\] \\[ \\text{Gamma prior: } f( \\lambda; \\alpha^*, \\beta^*) = \\frac{{\\beta^*}^{\\alpha^*}}{\\Gamma(\\alpha^*)} \\lambda^{\\alpha^* - 1} e^{-\\beta^*\\lambda} \\] \\[ \\begin{aligned} \\text{Gamma posterior: } f( \\lambda \\mid x) &amp;= \\frac{\\left( \\lambda^N e^{-\\lambda \\sum_{n = 1}^N x_n}\\right) \\times \\left( \\left[ \\frac{{\\beta^*}^{\\alpha^*}}{\\Gamma(\\alpha^*)} \\right] \\lambda^{\\alpha^* - 1} e^{-\\beta^*\\lambda}\\right)}{C_1} \\\\ &amp;= \\frac{\\left( \\lambda^N e^{-\\lambda \\sum_{n = 1}^N x_n}\\right) \\times \\left( \\lambda^{\\alpha^* - 1} e^{-\\beta^*\\lambda}\\right)}{C_2} \\\\ &amp;= \\frac{ \\lambda^{ \\overbrace{\\left[ \\alpha^* + N \\right]}^{\\alpha^\\prime} - 1} e^{- \\overbrace{\\left[ \\beta^* + \\sum_{n = 1}^N x_n \\right]}^{\\beta^\\prime} \\lambda}}{C_2} \\\\ &amp; = \\frac{{\\beta^\\prime}^{\\alpha^\\prime}}{\\Gamma(\\alpha^\\prime)} \\lambda^{\\alpha^\\prime - 1} e^{-\\beta^\\prime\\lambda}\\text{ where } \\alpha^\\prime = \\alpha^* + N \\text{ and } \\beta^\\prime = \\beta^* + \\sum_{n = 1}^N x_n \\end{aligned} \\] Exercise 2.2 You are a researcher interesting in government duration in parliamentary democracies. You decide to model duration as an exponential distribution. To estimate the parameter \\(\\lambda\\) of the exponential distribution, you collect data on the last five UK governments. The first Johnson ministry lasted 142 days. The second May ministry lasted 773 days. The first May ministry lasted 333 days. The second Cameron ministry lasted 432 days. The first Cameron ministry (Cameron-Clegg coalition) lasted 1,823 days. Rescale the data to years–that helps with the interpretation. Create three gamma prior distributions. One that describes your prior beliefs. One that’s weakly informative. One that’s as uninformative as possible. Use what you learned from Exercise 2.1 to plot each prior and posterior. For each posterior, compute the mean, median, and mode of \\(\\lambda\\). Interpret. Hint: \\(\\lambda\\) is a rate. If the durations are measured in years, then it’s the failures per year. The expected duration in years is \\(\\frac{1}{\\lambda}\\), but remember that \\(E \\left( \\frac{1}{\\lambda}\\right) \\neq \\frac{1} {E(\\lambda)}\\). If you want to find the posterior distribution of the expected duration (rather than the failures/year), then you can simulate many draws of \\(\\lambda\\) from the posterior and transform each draw into \\(\\frac{1}{\\lambda}\\). The mean of the transformed draws is the posterior mean of the expected duration. Assess the model. Do you think we have a good model? Hint: the exponential distribution is memoryless. Does that make sense in this appliation? Solutions # load packages library(tidyverse) library(stringr) library(kableExtra) # set seed for reproducibility set.seed(1234) # choose informative prior distribution # - I expect govts to last, on avg, 3 years, so fail at a # rate of lambda = 1/3 per year, give-or-take about 1/10. # - I know that the mean of the gamma is a/b and the sd is # sqrt{a/(b^2)}, so I set a/b = 1/3 and sqrt{a/(b^2)} = 1/10 # and solve. You could also just experiment with different # values alpha_inf &lt;- 100/9 beta_inf &lt;- 3*alpha_inf # plot informative prior ggplot() + xlim(0, 1) + stat_function(fun = dgamma, args = list(shape = alpha_inf, rate = beta_inf)) + labs(title = &quot;prior distribution&quot;, x = &quot;lambda; expected failures/year&quot;, y = &quot;prior density&quot;) # choose **weakly informative** prior distribution # - For this, I repeated the same process, but doubled my # give-or-take from 1/10 to 1/5, alpha_weak &lt;- 25/9 beta_weak &lt;- 3*alpha_weak # plot weakly informative prior ggplot() + xlim(0, 1) + stat_function(fun = dgamma, args = list(shape = alpha_weak, rate = beta_weak)) + labs(title = &quot;prior distribution&quot;, x = &quot;lambda; expected failures/year&quot;, y = &quot;prior density&quot;) # choose flattest prior distribution # - I just want to make this prior as close to uniform as possible. # - the failure rate (failures/year) is *surely* between 0 and 100 # (lambda = 100 means that 100 governments are failing per year), # so I plot the prior from zero to 100--it&#39;s basically flat alpha_flat &lt;- 1 beta_flat &lt;- 0.01 # plot flattish prior ggplot() + xlim(0, 100) + ylim(0, NA) + stat_function(fun = dgamma, args = list(shape = alpha_flat, rate = beta_flat)) + labs(title = &quot;prior distribution&quot;, x = &quot;lambda; expected failures/year&quot;, y = &quot;prior density&quot;) # data x &lt;- c(142, 773, 333, 432, 1823)/365 # rescale to years # make a data frame with the posterior and prior parameters posts &lt;- data.frame(prior_type = c(&quot;Informative&quot;, &quot;Weakly Informative&quot;, &quot;Flat-ish&quot;), alpha_prior = c(alpha_inf, alpha_weak, alpha_flat), beta_prior = c(beta_inf, beta_weak, beta_flat)) %&gt;% # add posterior parameters mutate(alpha_posterior = alpha_prior + length(x), beta_posterior = beta_prior + sum(x)) %&gt;% # create one row per parameter pivot_longer(cols = alpha_prior:beta_posterior) %&gt;% # separate parameter from distribution type separate(name, into = c(&quot;parameter&quot;, &quot;distribution_type&quot;)) %&gt;% # put parameters into separate columns pivot_wider(names_from = parameter, values_from = value) %&gt;% mutate(distribution_type = str_to_title(distribution_type), distribution_type = factor(distribution_type, levels = c(&quot;Prior&quot;, &quot;Posterior&quot;))) %&gt;% mutate(prior_type = factor(prior_type, levels = c(&quot;Informative&quot;, &quot;Weakly Informative&quot;, &quot;Flat-ish&quot;))) # compute point estimates and make a table point_estimates &lt;- posts %&gt;% #filter(distribution_type == &quot;posterior&quot;) %&gt;% group_by(prior_type, distribution_type) %&gt;% mutate(mean = alpha/beta, median = median(rgamma(100000, alpha, beta)), mode = (alpha - 1)/beta, mean_expected_duration = mean(1/rgamma(100000, shape = alpha, rate = beta))) point_estimates %&gt;% select(-alpha, -beta) %&gt;% arrange(distribution_type, prior_type) %&gt;% rename(`Prior` = prior_type, `Distribution` = distribution_type, `Mean of lambda` = mean, `Median of lambda` = median, `Mode of lambda` = mode, `Mean of 1/lambda` = mean_expected_duration) %&gt;% kable(format = &quot;markdown&quot;, digits = 2) Prior Distribution Mean of lambda Median of lambda Mode of lambda Mean of 1/lambda Informative Prior 0.33 0.32 0.30 3.30 Weakly Informative Prior 0.33 0.29 0.21 4.70 Flat-ish Prior 100.00 69.38 0.00 0.12 Informative Posterior 0.38 0.37 0.35 2.84 Weakly Informative Posterior 0.43 0.42 0.38 2.65 Flat-ish Posterior 0.62 0.59 0.52 1.92 # plot distributions gg_posts &lt;- posts %&gt;% # add in lambda values for which to compute the prior and posterior density full_join(data_frame(lambda = seq(0, 2, length.out = 100)), by = character()) %&gt;% # compute the posterior and prior density for each lambda mutate(density = dgamma(lambda, shape = alpha, rate = beta)) ggplot(gg_posts, aes(x = lambda, y = density, color = prior_type, linetype = distribution_type)) + geom_line() "],
["method-of-moments.html", "2.2 Method of Moments", " 2.2 Method of Moments 2.2.1 Mechanics Suppose a random variable \\(X\\). Then we refer to \\(E(X^k)\\) as the \\(k\\)-th moment of the distribution or population. Similarly, we refer to \\(\\text{avg}(x^k)\\) as the \\(k\\)-th sample moment. For example, recall that \\(V(X) = E \\left(X^2 \\right) - \\left[ E(X)\\right]^2\\). In example the variance of \\(X\\) is the difference between the second moment and the square of the first moment. Recall that the law of large numbers guarantees that \\(\\text{avg}(x) \\xrightarrow[]{d} E(X)\\). Thus, the first sample moment (the average) converges in probability to the first moment of \\(f\\) (the expected value or mean). By the law of the unconscious statistician, we can similarly guarantee that \\(\\text{avg}(x^k) \\xrightarrow[]{d} E(X^k)\\). Thus, the sample moments converge in distribution to moments of \\(f\\). Now suppose that \\(f\\) has parameters \\(\\theta_1, \\theta_2, ..., \\theta_k\\) so that \\(X \\sim f(\\theta_1, \\theta_2, ..., \\theta_k)\\). We know (or can solve) for the moments of \\(f\\) so that \\(E(X^1) = g_1(\\theta_1, \\theta_2, ..., \\theta_k)\\), \\(E(X^2) = g_2(\\theta_1, \\theta_2, ..., \\theta_k)\\), and so on. To use the method of moments, set the first \\(k\\) sample moments equal to the first \\(k\\) moments of \\(f\\) and relabel \\(\\theta_i\\) as \\(\\hat{\\theta}_i\\). Solve the system of equations for each \\(\\hat{\\theta}_i\\). 2.2.2 Example: Gamma Distribution Suppose we have \\(N\\) samples with replacement \\(x_n\\) for \\(n \\in \\{1, 2, ..., N\\}\\) from a \\(\\text{gamma}(\\alpha, \\beta)\\) distribution. We can use the method of moments to derive estimators of \\(\\alpha\\) and \\(\\beta\\). We have two parameters, so we use the first two moments of \\(f\\). For the gamma distribution, \\(E(X) = \\frac{\\alpha}{\\beta}\\) and \\(E(X^2) = V(X) + E(X)^2 = \\frac{\\alpha}{\\beta^2} + \\left( \\frac{\\alpha}{\\beta} \\right)^2 = \\frac{\\alpha(\\alpha + 1)}{\\beta^2}\\). Since we have two parameters, we set the first two sample moments equal to the first two moments of the gamma distribution, add hats, and solve. \\[ \\begin{aligned} \\displaystyle \\frac{1}{N} \\sum_{n = 1}^{N} x_n &amp;= \\frac{\\hat{\\alpha}}{\\hat{\\beta}}\\\\ \\displaystyle \\frac{1}{N} \\sum_{n = 1}^{N} x_n^2 &amp;= \\frac{\\hat{\\alpha}(\\hat{\\alpha} + 1)}{\\hat{\\beta}^2}\\\\ \\end{aligned} \\] For simplicity, let \\(m_1 = \\frac{1}{N} \\sum_{n = 1}^{N} x_n\\) and \\(m_2 = \\frac{1}{N} \\sum_{n = 1}^{N} x_n^2\\). Then, solving for \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\), we obtain \\[ \\begin{aligned} \\displaystyle \\hat{\\alpha} &amp;= \\frac{m_1^2}{m_2 - m_1^2}\\text{ and }\\\\ \\displaystyle \\hat{\\beta} &amp;= \\frac{m_1}{m_2 - m_1^2}\\text{.}\\\\ \\end{aligned} \\] 2.2.3 Example: Toothpaste Cap Problem We have a toothpaste cap–one with a wide bottom and a narrow top. We’re going to toss the toothpaste cap. It can either end up lying on its side, its (wide) bottom, or its (narrow) top. We want to estimate the probability of the toothpaste cap landing on its top. We can model each toss as a Bernoulli trial, thinking of each toss as a random variable \\(X\\) where \\(X \\sim \\text{Bernoulli}(\\pi)\\). If the cap lands on its top, we think of the outcome as 1. If not, as 0. Suppose we toss the cap \\(N\\) times and observe \\(k\\) tops. How can we estimate the parameter \\(\\pi\\)? We can use the method of moments to estimate this single parameter. Set the first moment of the sample to the first moment of the Bernoulli distribution. Add a hat to the quantities to estimate. Solve. This process is nearly trivial for the Bernoulli distribution. \\(\\text{sample average} = \\frac{k}{N} = \\hat{\\pi}\\). Thus, the fraction of the draws that land on their top is the methods of moments estimator of the probability of getting a top. 2.2.4 Example: Population Average Suppose we have \\(N\\) samples with replacement \\(x_n\\) for \\(n \\in \\{1, 2, ..., N\\}\\) from a population. We want to estimate the population average. To estimate the population average, we would simply set the first sample moment \\(m_1 = \\displaystyle \\frac{1}{n} \\sum_{n = 1}^{N} x_n^1\\) equal to the first population moment \\(\\mu_1 = E(X^1)\\). Then we equate the sample moments to the population moments, add hats to the quantities of interest, and solve. Because \\(E(X^1)\\) it the population average, there’s not much work to do. \\[ \\begin{aligned} m_1 &amp;= \\mu_1\\\\ \\displaystyle \\frac{1}{N} \\sum_{n = 1}^{N} x_n &amp;= \\widehat{E(X)}\\\\ &amp;= \\text{estimate of population average}\\\\ \\end{aligned} \\] Once we solve for the quantity of interest, we can use that solution to estimate the quantity of interest. In this example, the sample average \\(\\displaystyle \\frac{1}{n} \\sum_{n = 1}^{N} X_n\\) is methods of moments estimator of the population average. 2.2.5 Example: Population Average and SD Suppose we also want the population SD. We set the second sample moment \\(m_2 = \\displaystyle \\frac{1}{n} \\sum_{n = 1}^{N} x_n^2\\) equal to the second population moment \\(\\mu_2 = E(X^2)\\). Then we equate the sample moments to the population moments and solve for the quantities of interest. \\[ \\begin{aligned} \\displaystyle \\frac{1}{n} \\sum_{n = 1}^{N} x_n &amp;= \\widehat{E(X)}\\\\ \\displaystyle \\frac{1}{n} \\sum_{n = 1}^{N} x_n^2 &amp;= \\widehat{E(X^2)}\\\\ \\end{aligned} \\] Recall that the variance equals \\(V(X) = E \\left( X^2 \\right) - E \\left( X \\right)^2\\) and the population \\(\\text{SD}(X) = \\sqrt{V(X)}\\). \\[ \\begin{aligned} \\displaystyle \\sqrt{\\frac{1}{N} \\sum_{n = 1}^{N} x_n^2 - \\left[ \\displaystyle \\frac{1}{N} \\sum_{n = 1}^{N} x_n \\right]^2} &amp;= \\sqrt{\\widehat{E(X^2)} - \\widehat{E(X)}^2}\\\\ &amp;= \\text{estimate of population SD} \\end{aligned} \\] In this case, the sample SD is the methods of moments estimator for the population SD. Notes This is the same formula at the top of p. 74 of FPP. Some authors recommend using \\(N - 1\\) rather than \\(N\\), so that \\(\\sqrt{\\frac{1}{N - 1} \\sum_{n = 1}^{N} x_n^2 - \\left[ \\frac{1}{N - 1} \\sum_{n = 1}^{N} x_n \\right]^2}\\). However, the formula using \\(N - 1\\) is not the methods of moments estimator of the population SD. 2.2.6 Example: German Tank Problem In World War II, the Germans manufactured some unknown number of tanks \\(\\nu\\), but gave each a sequential serial number \\(S = \\{1, 2, ..., \\nu\\}\\). It’s of great strategic importance to know the \\(\\nu\\), the total number of tanks manufactured. When the Allies would destroy or capture a tank, they would record the serial number. Soon they had a data set of \\(N\\) observed serial numbers \\(\\{x_1, x_2, ..., x_N\\}\\), which they can treat as random without replacement from \\(S\\). Using the MOM, we can develop an estimator \\(\\hat{\\nu}_{MM}\\) of the parameter \\(\\nu\\). We have only a single parameter \\(\\nu\\), so we match just the first sample moment \\(\\frac{1}{N}\\sum x^1_i = \\text{avg}(x)\\) with the first population moment \\(E(X) = \\frac{\\nu + 1}{2}\\). Then we have \\[ \\begin{aligned} \\frac{1}{N} \\sum_{n = 1}^{N} x_n &amp;= \\frac{\\hat{\\nu}_{MM} + 1}{2}\\\\ \\frac{2}{N} \\sum_{n = 1}^{N} x_n &amp;= \\hat{\\nu}_{MM} + 1\\\\ \\frac{2}{N} \\left( \\sum_{n = 1}^{N} x_n \\right) - 1&amp; = \\hat{\\nu}_{MM} \\end{aligned} \\] Here’s a small simulation to demonstrate the estimator in repeated samples # the estimator calc_nu_hat &lt;- function(x) { N &lt;- length(x) sum_x &lt;- sum(x) nu_hat &lt;- ((2 / N) * sum_x) - 1 return(nu_hat) } # simulation parameters n_sims &lt;- 5 # the number of repeated samples N &lt;- 5 # the number of observations in each sample nu &lt;- 100 # the true value of nu # do the simulation set.seed(12345) for (i in 1:n_sims) { # take the ith sample x &lt;- sample(1:nu, size = N, replace = FALSE) x &lt;- sort(x) # sort x for convenience # compute the estimate nu_hat &lt;- calc_nu_hat(x) # print the results data &lt;- paste0(stringr::str_pad(x, 3, pad = &quot;0&quot;), collapse = &quot;, &quot;) estimate &lt;- paste0(&quot;nu_hat = &quot;, round(nu_hat, 2)) cat(&quot;Sim. #&quot;, i, &quot;: &quot;, data, &quot; --&gt; &quot;, estimate, &quot;\\n&quot;, sep = &quot;&quot;) } ## Sim. #1: 014, 051, 080, 090, 092 --&gt; nu_hat = 129.8 ## Sim. #2: 024, 058, 075, 093, 096 --&gt; nu_hat = 137.4 ## Sim. #3: 002, 038, 075, 086, 088 --&gt; nu_hat = 114.6 ## Sim. #4: 010, 032, 040, 081, 094 --&gt; nu_hat = 101.8 ## Sim. #5: 001, 030, 038, 039, 076 --&gt; nu_hat = 72.6 Take a look at the 5 data sets above and the associated MOM estimates. Supposing you didn’t know \\(\\nu\\), would these estimates seem reasonable? Can you spot any problems? (Look closely at Simulation #5.) 2.2.7 Remarks The method of moments has some advantages: MOM intuitive; it just makes sense that the sample mean is a reasonable estimate of the population mean. MOM is numerically easy, usually. Because we’re just computing (and sometimes then transforming) sample moments, we can usually compute these estimates quickly. MOM demands few assumptions. We don’t need to define the distribution of the data. It’s sufficient, for example, to declare that it has a mean \\(mu\\) and a variance \\(\\sigma^2\\) to obtain MOM estimator of these quantities. However, the methods three disadvantages: MOM estimators are rarely better and sometimes worse than maximum likelihood estimators. MOM estimates sometimes fall outside the logically possible parameter space (see Simulation #5 of the German tank problem above) and sometimes outside the parameter space all together (e.g., negative estimates of parameters that can only be positive). Because of the disadvantages, researchers rarely use the method of moments. However, they’re a general, intuitive principle here: (functions of) sample moments are reasonable estimates of (functions of) population moments. In particular, \\(\\frac{1}{n} \\sum_{n = 1}^{N} x_n\\) is a reasonable estimate of a population mean, and \\(\\sqrt{\\frac{1}{N} \\sum_{n = 1}^{N} x_n^2 - \\left[ \\frac{1}{N} \\sum_{n = 1}^{N} x_n \\right]^2}\\) is a reasonable estimate of the population SD. 2.2.8 Exercises Exercise 2.3 Suppose we collect \\(N\\) random samples \\(x = \\{x_1, x_2, ..., x_N\\}\\) and model each draw as a random variable \\(X \\sim \\text{expontial}(\\lambda)\\) with pdf \\(f(x_n | \\lambda) = \\lambda e^{-\\lambda x_n}\\). Find the method of moments estimator of \\(\\lambda\\). Hint: the mean of the exponential distribution is $. Solution Since we have a single parameter to estimate, we just set the first sample moment (the average) equal to the first moment of the distribution (the mean; \\(\\frac{1}{\\lambda}\\)) and add a hat. Thus, \\(\\text{avg}(x) = \\frac{1}{\\hat{\\lambda}}\\) and therefore \\(\\hat{\\lambda} = \\frac{1}{\\text{avg}(x)}\\). Exercise 2.4 Model the data from Exercise 2.2 as draws from an exponential distribution with rate parameter \\(\\lambda\\). Use the method of moments estimator from Exercise 2.3 to estimate \\(\\lambda\\). What if you want to estimate the mean duration \\(\\mu = \\frac{1}{\\lambda}\\) rather than the failure rate \\(\\lambda\\)? Can you just use \\(\\hat{\\mu} = \\frac{1}{\\hat{\\lambda}}\\)? Solution x &lt;- c(142, 773, 333, 432, 1823)/365 # rescale to years 1/mean(x) # mm estimator of lambda ## [1] 0.520982 By construction, MOM estimators are invariant under transformation, so the MOM estimate of \\(g(\\theta)\\) can be found by transforming the MOM estimate of \\(\\theta\\), so \\(\\widehat{g(\\theta)} = g(\\hat{\\theta})\\). For this problem, \\(\\hat{\\mu} = \\frac{1}{\\hat{\\lambda}}\\). "],
["maximum-likelihood.html", "2.3 Maximum Likelihood", " 2.3 Maximum Likelihood 2.3.1 Mechanics Suppose we have a random sample from a distribution \\(f(x \\mid \\theta)\\). We find the maximum likelihood (ML) estimator \\(\\hat{\\theta}\\) of \\(\\theta\\) by maximizing the likelihood of the observed data with respect to \\(\\theta\\). In short, we take the likelihood from Section 2.1.1.1 and find the parameter \\(\\theta\\) that maximizes it. In practice, to make the math and/or computation a bit easier, we manipulate the likelihood function in three ways: Relabel the likelihood function \\(f(x \\mid \\theta) = L(\\theta)\\), since it’s weird to maximize with respect to a conditioning variable. Work with \\(\\log L(\\theta)\\) rather than \\(L(\\theta)\\). Because \\(\\log()\\) is a monotonically increasing function, the \\(\\theta\\) that maximizes \\(L(\\theta)\\) also maximizes \\(\\log L(\\theta)\\). Suppose we have samples \\(x_1, x_2, ..., x_N\\) from \\(f(x \\mid \\theta)\\). Then the likelihood is \\(f(x \\mid \\theta) = \\prod_{n = 1}^N f(x_n \\mid \\theta)\\) and \\(\\log L(\\theta) = \\sum_{n = 1}^N \\log \\left[ f(x_n \\mid \\theta) \\right]\\). The ML estimator \\(\\hat{\\theta}\\) of \\(\\theta\\) is \\(\\arg \\max \\log L(\\theta)\\). In applied problems, we might be able to simplify \\(\\log L\\) substantially. Occasionally, we can find a nice analytical maximum. In many cases, we have a computer find the parameter that maximizes \\(\\log L\\). 2.3.2 Example: Toothpaste Cap Problem For the toothpaste cap problem, we have the following likelihood, which I’m borrowing directly from Section 2.1.1.1. \\[ \\text{the likelihood: } f(x | \\pi) = \\pi^{k} (1 - \\pi)^{(N - k)}, \\text{where } k = \\sum_{n = 1}^N x_n \\\\ \\] Then, we relabel. \\[ \\text{the likelihood: } L(\\pi) = \\pi^{k} (1 - \\pi)^{(N - k)}\\\\ \\] Then, we take the log and simplify. \\[ \\text{the likelihood: } \\log L(\\pi) = k \\log (\\pi) + (N - k) \\log(1 - \\pi)\\\\ \\] To find the ML estimator, we find \\(\\hat{\\pi}\\) that maximizes \\(\\log L\\). In this case, the analytical optimum is easy. \\[ \\begin{aligned} \\frac{d \\log L}{d\\hat{\\pi}} = y \\left( \\frac{1}{\\hat{\\pi}}\\right) + (N - y) \\left( \\frac{1}{1 - \\hat{\\pi}}\\right)(-1) &amp;= 0\\\\ \\frac{y}{\\hat{\\pi}} - \\frac{N - y}{1 - \\hat{\\pi}} &amp;= 0 \\\\ \\frac{y}{\\hat{\\pi}} &amp;= \\frac{N - y}{1 - \\hat{\\pi}} \\\\ y(1 - \\hat{\\pi}) &amp;= (N - y)\\hat{\\pi} \\\\ y - y\\hat{\\pi} &amp;= N\\hat{\\pi} - y\\hat{\\pi} \\\\ y &amp;= N\\hat{\\pi} \\\\ \\hat{\\pi} &amp;= \\frac{y}{N} = \\text{avg}(x)\\\\ \\end{aligned} \\] The ML estimator for the Bernoulli distribution is the fraction of successes, or, equivalently, the average of the Bernoulli trials. The collected data consist of 150 trials and 8 successes, so the ML estimate of \\(\\pi\\) is \\(\\frac{8}{150} \\approx 0.053\\). Compare the ML estimate with the posterior mean, median, and mode above. 2.3.3 Example: German Tank Problem The German tank problem is an excellent example of an ML estimator because one can think through the logic intuitively rather than mathematically. Recall that the Allies observe \\(N\\) serial numbers of German tanks. By treating these serial numbers as samples without replacement from a set of sequential serial numbers \\(S = \\{1, 2, ..., \\nu\\}\\), they can estimate the total number of German tanks \\(\\nu\\). Because we’re sampling without replacement, the likelihood is quite complicated. \\[ f(x \\mid \\nu) = f(x_1) f(x_2 \\mid x_1) ... f(x_N | x_{N - 1}, ..., x_2, x_1), \\\\ \\text{where } x_i \\neq x_j \\text{ for } i \\neq j. \\] Remember that \\(\\nu\\) is the total number of tanks. So the chance that we observe \\(x_1\\) first is \\(\\frac{1}{\\nu}\\). The chance we observe \\(x_2\\) second, given that we observe \\(x_1\\) first is \\(\\frac{1}{\\nu - 1}\\). The chance we observe \\(x_N\\) \\(N\\)th is \\(\\frac{1}{\\nu - (N - 1)}\\). \\[ L(\\nu) = \\frac{1}{\\nu} \\times \\frac{1}{\\nu - 1} \\times ... \\times \\frac{1}{\\nu - (N - 1)} \\] It’s clear, then, that in order to make \\(L\\) as large as possible, we need to make \\(\\nu\\) as small as possible. However, notice that \\(\\nu\\) cannot be less than \\(\\max \\{x_1, ..., x_N\\}\\) (i.e., the largest serial number must be greater than or equal to the largest observed serial number). So what’s the smallest value of \\(\\nu\\) that satisfies \\(\\nu \\geq \\max \\{x_1, ..., x_N\\}\\)? Of course it’s \\(\\hat{\\nu} = \\max \\{x_1, ..., x_N\\}\\). To illustrate, I simulate the same five data sets from above and compute the MM and ML estimates. Take a look at the five data sets and five estimates. Would you say that one estimator seems better? Can the ML estimate ever be too large? Can the MM estimate ever be too small? # the estimators calc_nu_hat_mm &lt;- function(x) { N &lt;- length(x) sum_x &lt;- sum(x) nu_hat &lt;- ((2 / N) * sum_x) - 1 return(nu_hat) } # the estimators calc_nu_hat_ml &lt;- function(x) { nu_hat &lt;- max(x) return(nu_hat) } # simulation parameters n_sims &lt;- 5 # the number of repeated samples N &lt;- 5 # the number of observations in each sample nu &lt;- 100 # the true value of nu # do the simulation set.seed(12345) for (i in 1:n_sims) { # take the ith sample x &lt;- sample(1:nu, size = N, replace = FALSE) x &lt;- sort(x) # sort x for convenience # compute the estimates nu_hat_mm &lt;- calc_nu_hat_mm(x) nu_hat_ml &lt;- calc_nu_hat_ml(x) # print the results data &lt;- paste0(stringr::str_pad(x, 3, pad = &quot;0&quot;), collapse = &quot;, &quot;) mm_estimate &lt;- paste0(&quot;MM estimate = &quot;, round(nu_hat_mm, 2)) ml_estimate &lt;- paste0(&quot;ML estimate = &quot;, round(nu_hat_ml, 2)) cat(&quot;Sim. #&quot;, i, &quot;: &quot;, data, &quot; --&gt; &quot;, ml_estimate, &quot;; &quot;, mm_estimate, &quot;\\n&quot;, sep = &quot;&quot;) } ## Sim. #1: 014, 051, 080, 090, 092 --&gt; ML estimate = 92; MM estimate = 129.8 ## Sim. #2: 024, 058, 075, 093, 096 --&gt; ML estimate = 96; MM estimate = 137.4 ## Sim. #3: 002, 038, 075, 086, 088 --&gt; ML estimate = 88; MM estimate = 114.6 ## Sim. #4: 010, 032, 040, 081, 094 --&gt; ML estimate = 94; MM estimate = 101.8 ## Sim. #5: 001, 030, 038, 039, 076 --&gt; ML estimate = 76; MM estimate = 72.6 2.3.4 Example: Poisson Distribution Suppose we collect \\(N\\) random samples \\(x = \\{x_1, x_2, ..., x_N\\}\\) and model each draw as a random variable \\(X \\sim \\text{Poisson}(\\lambda)\\). Find the ML estimator of \\(\\lambda\\). \\[ \\begin{aligned} \\text{Poisson likelihood: } f(x \\mid \\lambda) &amp;= \\prod_{n = 1}^N \\frac{\\lambda^{x_n} e^{-\\lambda}}{x_n!} \\\\ L(\\lambda) &amp;= \\prod_{n = 1}^N \\frac{\\lambda^{x_n} e^{-\\lambda}}{x_n!} \\\\ \\log L(\\lambda) &amp;= \\sum_{n = 1}^N \\log \\left[ \\frac{\\lambda^{x_n} e^{-\\lambda}}{x_n!} \\right]\\\\ &amp;= \\sum_{n = 1}^N \\left[ x_n \\log \\lambda + (-\\lambda) \\log e - \\log x_n! \\right]\\\\ &amp;= \\log \\lambda \\left[ \\sum_{n = 1}^N x_n \\right] -N\\lambda + \\sum_{n = 1}^N \\log (x_n!) \\\\ \\end{aligned} \\] To find the ML estimator, we find \\(\\hat{\\lambda}\\) that maximizes \\(\\log L\\). In this case, the analytical optimum is easy. \\[ \\begin{aligned} \\frac{d \\log L}{d\\hat{\\lambda}} = \\frac{1}{\\hat{\\lambda}} \\left[ \\sum_{n = 1}^N x_n \\right] - N &amp;= 0\\\\ \\frac{1}{\\hat{\\lambda}} \\left[ \\sum_{n = 1}^N x_n \\right] &amp;= N \\\\ \\left[ \\sum_{n = 1}^N x_n \\right] &amp;= N \\hat{\\lambda} \\\\ \\hat{\\lambda} &amp;= \\frac{ \\sum_{n = 1}^N x_n }{N} = \\text{avg}(x) \\\\ \\end{aligned} \\] The ML estimator for the Poisson distribution is just the average of the samples. 2.3.5 Remarks The ML estimator is extremely common in political science because they are general, fast, and work extremely well. Lots of models that you’ve heard of, such as logistic regression, are estimated with ML. We can even obtain ML estimates for the linear regression model. We assume that the observed data are samples from a normal distribution with mean \\(\\mu_n = \\alpha + \\beta x_n\\) and variance \\(\\sigma^2\\). For this model, the least-squares estimate that we learned earlier is also the ML estimates. 2.3.6 Exercises Exercise 2.5 Suppose we collect \\(N\\) random samples \\(x = \\{x_1, x_2, ..., x_N\\}\\) and model each draw as a random variable \\(X \\sim \\text{expontial}(\\lambda)\\) with pdf \\(f(x_n | \\lambda) = \\lambda e^{-\\lambda x_n}\\). Find the maximum estimator of \\(\\lambda\\). Solution The math follows the Poisson example closely. However, the solution is the inverse–\\(\\hat{\\lambda} = \\frac{N}{\\sum_{n = 1}^N x_n } = \\frac{1}{\\text{avg}(x)}\\). Exercise 2.6 Model the data from Exercise 2.2 as draws from an exponential distribution with rate parameter \\(\\lambda\\). Use the maximum estimator from Exercise 2.5 to estimate \\(\\lambda\\). What if you want to estimate the mean duration \\(\\mu = \\frac{1}{\\lambda}\\) rather than the failure rate \\(\\lambda\\)? Can you just use \\(\\hat{\\mu} = \\frac{1}{\\hat{\\lambda}}\\)? Solution x &lt;- c(142, 773, 333, 432, 1823)/365 # rescale to years 1/mean(x) # ml estimator of lambda ## [1] 0.520982 Like MM estimators, ML estimators are invariant under transformation, though this is not as obvious as it is for MM estimators. For this problem, \\(\\hat{\\mu} = \\frac{1}{\\hat{\\lambda}}\\). "],
["evaluating-point-estimates.html", "Chapter 3 Evaluating Point Estimates", " Chapter 3 Evaluating Point Estimates In this chapter, I discuss three concepts that we can use to evaluate an estimator from a frequentist perspective. Bias Consistency MVUE or BUE As a running example, we have the toothpaste cap problem and the following estimators of the chance of getting a top. posterior mean: \\(\\hat{\\pi}^{Bayes} = \\dfrac{\\alpha^\\prime}{\\alpha^\\prime + \\beta^\\prime} = \\dfrac{\\alpha^* + k}{[\\alpha^* + k] + [\\beta^* + (N - k)]} = \\dfrac{\\alpha^* + k}{\\alpha^* + \\beta^* + N }\\) method of moments estimator: \\(\\hat{\\pi}^{MM} = \\frac{k}{n}\\) maximum likelihood estimator: \\(\\hat{\\pi}^{ML} = \\frac{k}{n}\\) For each of these estimators we can ask: Is it good? (in an absolute sense) Is it better than another estimator? "],
["bias.html", "3.1 Bias", " 3.1 Bias Imagine repeatedly sampling and computing the estimate \\(\\hat{\\theta}\\) of the parameter \\(\\theta\\) for each sample. In this thought experiment, \\(\\hat{\\theta}\\) is a random variable. We say that \\(\\hat{\\theta}\\) is biased if \\(E(\\hat{\\theta}) \\neq \\theta\\). We say that \\(\\hat{\\theta}\\) is unbiased if \\(E(\\hat{\\theta}) = \\theta\\). We say that the bias of \\(\\hat{\\theta}\\) is \\(E(\\hat{\\theta}) - \\theta\\). For example, we can compute the bias of our ML estimator of \\(\\pi\\) in the toothpaste cap problem. \\[ \\begin{aligned} E\\left[ \\frac{k}{N}\\right] &amp;= \\frac{1}{N} E(k) = \\frac{1}{N} E \\overbrace{ \\left( \\sum_{n = 1}^N x_n \\right) }^{\\text{recall } k = \\sum_{n = 1}^N x_n } = \\frac{1}{N} \\sum_{n = 1}^N E(x_n) = \\frac{1}{N} \\sum_{n = 1}^N \\pi = \\frac{1}{N}N\\pi \\\\ &amp;= \\pi \\end{aligned} \\] Thus, \\(\\hat{\\pi}^{ML}\\) is an unbiased estimator of \\(\\pi\\) in the toothpaste cap problem. We need to be cautious about evaluating the frequentist properties of Bayesian estimates. Bayesian’s approach inference by describing prior beliefs and then updating those beliefs. This is a logical process. So long at the math is correct, we don’t really need to evaluate whether the posterior is “good” or “bad” or “better” or “worse”–it’s just the posterior. However, setting philosophical orientations aside, nothing prevents us from evaluting the frequentist properties of a posterior mean. \\[ \\begin{aligned} E\\left[ \\frac{\\alpha^* + k}{\\alpha^* + \\beta^* + N}\\right] &amp;= \\frac{1}{\\alpha^* + \\beta^* + N} E(k + \\alpha^*) = \\frac{1}{\\alpha^* + \\beta^* + N} \\left[ E(k) + \\alpha^* \\right] \\\\ &amp; = \\frac{1}{\\alpha^* + \\beta^* + N} \\left[ \\sum_{n = 1}^N E(x_n) + \\alpha^* \\right] \\\\ &amp; = \\frac{1}{\\alpha^* + \\beta^* + N} \\left[ \\sum_{n = 1}^N \\pi + \\alpha^* \\right] \\\\ &amp; = \\frac{N\\pi + \\alpha^*}{\\alpha^* + \\beta^* + N} \\end{aligned} \\] As \\(N \\xrightarrow{} \\infty\\), the expected value approaches \\(\\pi\\), but the posterior mean is biased in finite samples. Remember that both prior parameters must be positive. We get a nice, intuitive result, though. Remember that if \\(\\alpha = \\beta\\), then the beta distribution is symmetric about one-half. As the parameters grow larger, it becomes less variable around zero. You can see that for \\(\\alpha = \\beta &gt; 1\\), the posterior mean becomes more biased toward $# as \\(\\alpha\\) and \\(\\beta\\) grow larger. 3.1.1 Example: Sample Average Similarly, we can show that, for a simple random sample, the sample average is an unbiased estimate of the population average. \\[ \\begin{aligned} E\\left[ \\frac{\\sum_{n = 1}^N x_n}{N}\\right] &amp;= \\frac{1}{N} E\\left[ \\sum_{n = 1}^N x_n \\right] = \\frac{1}{N} \\sum_{n = 1}^N E(x_n) \\\\ &amp;= \\frac{1}{N} \\sum_{n = 1}^N \\text{(pop. avg.)} = \\frac{1}{N}N \\text{(pop. avg.)} \\\\ &amp; = \\text{pop. avg.} \\end{aligned} \\] 3.1.2 Example: Poisson Distribution Using math almost identical to the toothpaste cap problem, we can show that the ML estimator \\(\\hat{\\lambda} = \\text{avg}(x)\\) is an unbiased estimator of \\(\\lambda\\). We can also illustrate the unbiasedness with a computer simulation. lambda &lt;- 4.0 # the parameter we&#39;re trying to estimate sample_size &lt;- 10 # the sample size we&#39;re using in each &quot;study&quot; n_repeated_samples &lt;- 10000 # the number of times we repeat the &quot;study&quot; lambda_hat &lt;- numeric(n_repeated_samples) # a container for (i in 1:n_repeated_samples) { x &lt;- rpois(sample_size, lambda = lambda) lambda_hat[i] &lt;- mean(x) } # long-run average mean(lambda_hat) ## [1] 4.00389 "],
["consistency.html", "3.2 Consistency", " 3.2 Consistency Imagine take a sample of size N and compute the estimate \\(\\hat{\\theta}_N\\) of the parameter \\(\\theta\\). We say that \\(\\hat{\\theta}\\) is a consistent estimator of \\(\\theta\\) if \\(\\hat{\\theta}\\) converges in probability to \\(\\theta\\). Intuitively, this means the following: For a large enough sample, the estimator returns the exact right answer. For a large enough sample, the estimate \\(\\hat{theta}\\) does not vary any more, but collapses onto a single point and that point is \\(\\theta\\). Under weak, but somewhat technical, assumptions that usually hold, ML estimators are consistent. Under even weaker assumptions, MM estimators are consistent. Given that we always have finite samples, why is consistency valuable? In short, it’s not valuable, directly. However, consistent estimators tend to be decent with small samples. But it does not follow that consistent estimators work well in small samples. Consider the estimator \\(\\hat{\\pi}^{Bayes}\\). By appropriate (i.e., large) values for \\(\\alpha^*\\) and \\(\\beta^*\\), we can make the \\(E(\\hat{\\pi}^{Bayes})\\) whatever we like in the \\((0, 1)\\) interval. But \\(E(\\hat{\\pi}^{Bayes})\\) is consistent regardless of the values we choose for \\(\\alpha^*\\) and \\(\\beta^*\\). Even though posterior mean is consistent, it can be highly biased for finite samples. However, as a rough guideline, consistent estimators work well for small samples. However, whether they actually work well in any particular situation needs a more careful investigation. "],
["mvue-or-bue.html", "3.3 MVUE or BUE", " 3.3 MVUE or BUE Imagine repeatedly sampling and computing the estimate \\(\\hat{\\theta}\\) of the parameter \\(\\theta\\) for each sample. In this thought experiment, \\(\\hat{\\theta}\\) is a random variable. We refer to \\(E \\left[ (\\hat{\\theta} - \\theta)^2 \\right]\\) as the **mean-squared error* (MSE) of \\(\\hat{\\theta}\\). Some people use the square-root of the MSE, which we refer to as the root-mean-squared error (RMSE). In general, we prefer estimators with a smaller MSE to estimators with a larger MSE. Notice that an estimator can have a larger MSE because (1) it’s more variable or (2) more biased. To see this, we can decompose the MSE into two components. \\[ \\begin{aligned} E \\left[ (\\hat{\\theta} - \\theta)^2 \\right] &amp;= E \\left[ (\\hat{\\theta} - E(\\hat{\\theta})\\right] ^2 + E(\\hat{\\theta} - \\theta)^2\\\\ \\text{MSE}(\\hat{\\theta}) &amp;= \\text{Var}(\\hat{\\theta}) + \\text{Bias}(\\hat{\\theta})^2 \\end{aligned} \\] When designing an estimator, we usually follow this process: Eliminate biased estimators, if an unbiased estimator exists. Among the remaining unbiased estimators, select the one with the smallest variance. (The variance equls the MSE for unbiased estimators.) This process does not necessarily result in the estimator with the smallest MSE, but it does give us the estimator with the smallest MSE among the unbiased estimators. This seems tricky—how do we know we’ve got the estimator with the smallest MSE among the unbiased estimator? Couldn’t there always be another, better unbiased estimator that we haven’t considered? It turns out that we have a theoretical lower bound on the variance of an estimator. No unbiased estimator can have a variance below the Cramér-Rao Lower Bound. If an unbiased estimator equals the Cramér-Rao Lower Bound, then we say that estimator attains the Cramér-Rao Lower Bound. We refer to an estimator that attains the Cramér-Rao Lower Bound as the minimum-variance unbiased estimator (MVUE) or the best unbiased estimator (BUE). A MVUE estimator is the gold standard. It is possible, though, that a biased alternative estimator has a smaller MSE. It’s beyond our scope to establish whether particular estimators are the MVUE. However, in general, the sample average is an MVUE of the expected value of a distribution. Alternatively, the sample average is the MVUE of the population average. If you are using \\(\\hat{\\theta} = \\text{avg}(x)\\) to estimate \\(\\theta = E(X)\\), then \\(\\hat{\\theta}\\) is an MVUE. "],
["exercises.html", "3.4 Exercises", " 3.4 Exercises Exercise 3.1 First, use a computer simulation to check whether the ML estimator \\(\\hat{\\lambda} = \\frac{1}{\\text{avg}(x)}\\) of \\(\\lambda\\) for the exponential distribution is (at least approximately) unbiased. (See the example for the Poisson distribution above.) Second, prove that the ML estimator of \\(\\lambda\\) is biased. Third, check whether the ML estimator \\(\\hat{\\mu} = \\frac{1}{\\hat{lambda}}\\) of \\(\\mu = \\frac{1}{\\lambda}\\) is biased. Explain the implications for the analysis of UK government duration. Solution lambda &lt;- 4.0 # the parameter we&#39;re trying to estimate sample_size &lt;- 10 # the sample size we&#39;re using in each &quot;study&quot; n_repeated_samples &lt;- 10000 # the number of times we repeat the &quot;study&quot; lambda_hat &lt;- numeric(n_repeated_samples) # a container for (i in 1:n_repeated_samples) { x &lt;- rexp(sample_size, rate = lambda) lambda_hat[i] &lt;- 1/mean(x) } # long-run average mean(lambda_hat) # hum... seems too big ## [1] 4.436498 \\[ \\begin{aligned} E\\left[ \\frac{1}{\\text{avg}(x)}\\right] = E\\left[ \\frac{N}{\\sum_{n = 1}^N x_n}\\right] &amp;= N E \\left[ \\frac{1}{ \\sum_{n = 1}^N x_n} \\right] \\\\ \\text{[notice the greater-than sign here]} \\rightarrow &amp; &gt; N \\left[ \\frac{1}{ E \\left( \\sum_{n = 1}^N x_n \\right) } \\right] \\text{, by Jensen&#39;s inquality; } \\frac{1}{x} \\text{ is convex.}\\\\\\\\ &amp; = N \\left[ \\frac{1}{ \\sum_{n = 1}^N E(x_n) } \\right] \\\\ &amp; = N \\left[ \\frac{1}{ \\sum_{n = 1}^N \\frac{1}{\\lambda} } \\right] \\\\ &amp; = N \\left[ \\frac{1}{ \\frac{N}{\\lambda} } \\right] \\\\ &amp; = \\lambda \\end{aligned} \\] Therefore the bias in \\(\\hat{\\lambda}\\) is upward, as the simulation shows. Remember that we can transform ML estimators directly to obtain estimates of the transformations. The ML estimator \\(\\hat{\\mu} = \\frac{1}{\\hat{\\lambda}}\\) of the average duration \\(\\mu = \\frac{1}{\\lambda} = E(X)\\) is unbiased, though. The math works nearly identically as the math showing that the ML estimator for the toothpaste cap problem. It also follows directly from the fact that the sample average is an unbiased estimator of the population average. Exercise 3.2 The ML estimator \\(\\hat{\\lambda} = \\frac{1}{\\text{avg}(x)}\\) of \\(\\lambda\\) for the exponential distribution is consistent. Comment on the relevance for Exercise @ref{exrml-govt-duration}. Solution Consistency is not helpful for this problem. We have five observations. At best, consistency suggests an estimator will work well in large samples. A consistent estimator can be heavily biased in small samples. Exercise 3.3 Comment on the ML estimator \\(\\hat{\\lambda} = \\frac{1}{\\text{avg}(x)}\\) of \\(\\lambda\\) for the exponential distribution and the ML estimator \\(\\hat{\\mu} = \\frac{1}{\\hat{\\lambda}}\\) of the \\(\\mu = \\frac{1}{\\lambda}\\). Comment on the relevance for Exercise @ref{exrml-govt-duration}. Solution Both estimators are consistent. However, consistency is not helpful in small samples. \\(\\hat{\\mu}\\) is unbiased, and \\(\\hat{\\lambda}\\) is biased. This leads us to prefer \\(\\hat{\\mu}\\). Further \\(\\hat{\\mu}\\) is the MVUE, because it’s the sample average used to estimate the expected value of the distribution. MVUE is an excellent estimator and it’s unusually to prefer another. \\(\\hat{\\mu}\\) has much better properties than \\(\\hat{\\lambda}\\)—\\(\\hat{\\mu}\\) is an MVUE, \\(\\hat{\\lambda}\\) is just consistent. Perhaps more importantly, \\(\\hat{\\mu}\\) is easier to interpret as the expected years a government will last. \\(\\hat{\\lambda}\\) is a bit harder to interpret at the failures per year. Here’s a little simulation illustrating each estimator. lambda &lt;- 0.5 # a guess at the value in the govt duration problem sample_size &lt;- 5 # the sample size in the govt duration problem n_repeated_samples &lt;- 10000 # the number of times we repeat the &quot;study&quot; lambda_hat &lt;- numeric(n_repeated_samples) # a container mu_hat &lt;- numeric(n_repeated_samples) # another container for (i in 1:n_repeated_samples) { x &lt;- rexp(sample_size, rate = lambda) lambda_hat[i] &lt;- 1/mean(x) mu_hat[i] &lt;- mean(x) } # long-run average mean(lambda_hat) # hum... seems too big ## [1] 0.6240386 mean(mu_hat) # just right! ## [1] 2.004443 library(ggplot2) library(patchwork) gg1 &lt;- qplot(x = lambda_hat) + geom_vline(xintercept = lambda) + geom_vline(xintercept = mean(lambda_hat), color = &quot;red&quot;) + labs(title = &quot;Distribution of lambda-hat in Repeated Samples&quot;, subtitle = &quot;Black Line Shows Actual lambda; Red Line Shows E(lambda-hat)&quot;) gg2 &lt;- qplot(x = mu_hat) + geom_vline(xintercept = 1/lambda) + geom_vline(xintercept = mean(mu_hat), color = &quot;red&quot;) + labs(title = &quot;Distribution of mu-hat in Repeated Samples&quot;, subtitle = &quot;Black Line Shows Actual mu; Red Line Shows E(lambda-mu)&quot;) gg1 + gg2 "]
]
